# CRAWLER

Program designed to get all the natural resources of a website, download them, analyse them...

## TO DO

- Adding the possibility to specify custom headers for every request
- Random agent possibility
- Option to download content
- Option to download react source content
- Option to output a file with the trees of the website
- Make "?" option being something we can highlight
- specify which data where belonging to
- Highlight particular data like localhost, 127.0.0.1, subdomains...
- Get all the comment (html comment, react comment...)
- Get all php code found and indicate in which file you got it
- Looking for api inside compiled javascript for client side javascript based 
- Looking for meta-data on every files
- Make a pourcentage of each files in a website
- Option d'inclure les sous-domaines
- Option de limiter le nombre de page Ã  aller voir
- Filter specific data in src, href like 'data:base64...', 'javascript:...', 'mailto: ...'
- Option of multithreading
- live updating trees
- interactive 
- put the absolute link in parenthesis and status code
- order keys inside the dictionnary
- filter directory and file with end files and depth inside the dictionnaray
- limit of the page seen 
- make possible to only accept the content of some kind of file 